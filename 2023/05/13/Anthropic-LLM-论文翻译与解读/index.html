<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Anthropic LLM 论文翻译与解读 | Joizhang&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="题目论文名为《Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback》，该论文发表于 2022-04-12，论文的作者由 31 位来自 Anthropic 的研究人员组成。题目直白的翻译是“通过基于人类反馈的强化学习来训练一个有益无害的助手”。 摘要摘要开门见山，直接介绍本文">
<meta property="og:type" content="article">
<meta property="og:title" content="Anthropic LLM 论文翻译与解读">
<meta property="og:url" content="https://joizhang.github.io/2023/05/13/Anthropic-LLM-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/index.html">
<meta property="og:site_name" content="Joizhang&#39;s blog">
<meta property="og:description" content="题目论文名为《Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback》，该论文发表于 2022-04-12，论文的作者由 31 位来自 Anthropic 的研究人员组成。题目直白的翻译是“通过基于人类反馈的强化学习来训练一个有益无害的助手”。 摘要摘要开门见山，直接介绍本文">
<meta property="og:locale">
<meta property="og:image" content="https://s2.loli.net/2023/06/07/oxe3IndHWTErGyU.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/07/DtVhHQCs9brekfn.png">
<meta property="og:image" content="https://s2.loli.net/2023/06/07/ujX8SfiTIA14bON.png">
<meta property="article:published_time" content="2023-05-13T09:31:27.000Z">
<meta property="article:modified_time" content="2023-06-07T12:58:39.723Z">
<meta property="article:author" content="Joizhang">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="RLHF">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/06/07/oxe3IndHWTErGyU.png">
  
    <link rel="alternate" href="/atom.xml" title="Joizhang&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Joizhang&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://joizhang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Anthropic-LLM-论文翻译与解读" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2023/05/13/Anthropic-LLM-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/" class="article-date">
  <time datetime="2023-05-13T09:31:27.000Z" itemprop="datePublished">2023-05-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Anthropic LLM 论文翻译与解读
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>论文名为《Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback》，该论文发表于 2022-04-12，论文的作者由 31 位来自 Anthropic 的研究人员组成。题目直白的翻译是“通过基于人类反馈的强化学习来训练一个有益无害的助手”。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>摘要开门见山，直接介绍本文采用偏好建模和基于人类反馈的强化学习(RLHF)的来微调语言模型，以充当有益和无害的助手。本文发现，这种对齐训练提高了几乎所有NLP评估的性能，并且与Python编码和摘要等专业技能的训练完全兼容。接着介绍本文的主要贡献：</p>
<ol>
<li>本文探索了一种迭代的在线训练模式，其中偏好模型和RL策略以每周一次的频率，并且采用最新的人类反馈数据更新，有效地改进了数据集和模型。<span id="more"></span></li>
<li>本文研究了RLHF训练的鲁棒性，并确定了RL奖励和策略与其初始化之间的KL散度的平方根之间的大致线性关系。</li>
<li>除了主要结果本文还对校准、竞争目标和Out-of-distribution检测的使用进行了外围分析，将本文模型与人类作者进行了比较，并提供了来自本文模型的使用最近相关工作中出现的提示的样本。</li>
</ol>
<p><img src="https://s2.loli.net/2023/06/07/oxe3IndHWTErGyU.png" alt="图 1 该图总结了众包工作者对各种模型的偏好，包括上下文提取模型、在本文的“静态”数据集上训练的RLHF模型，以及通过迭代“在线”方法训练的RLHF模型，用于有益无害(HH)或仅用于有益。与52B上下文提取模型相比，本文提供了Elo分数和众包人员偏好样本的频率匹配。对于有益和无害，更高的分数是更可取的。"></p>
<h2 id="第一节-引言"><a href="#第一节-引言" class="headerlink" title="第一节 引言"></a>第一节 引言</h2><p>本文希望开发技术来训练有帮助、诚实和无害的人工智能代理。在本文中，展示了通过收集人类偏好数据并应用偏好建模(preference modeling, PMing)以及使用来自人类反馈的强化学习(reinforcement learning from human feedback, RLHF)技术可以训练一个相对有帮助且无害的(helpful and harmless, HH)自然语言助手。图 2 总结了整个训练过程。</p>
<p><img src="https://s2.loli.net/2023/06/07/DtVhHQCs9brekfn.png" alt="图 2 此图总结了数据收集和模型训练工作流程。"></p>
<p>本文的目标不是定义或规定“有益的”和“无害的”意味着什么，而是评估本文训练技术的有效性，所以在大多数情况下，作者团队只是让他们的众包工作人员按照他们认为合适的方式来解释这些概念。本文区别对待有益和无害，为它们收集不同的人类偏好数据集。对于有益性，作者团队要求众包工作人员征求模型来协助任何纯粹基于文本的任务，例如回答问题、编写或编辑文档，或者讨论计划和决策。对于无害性，作者团队要求众包工作人员对语言模型进行对抗性探索或“红队”，以激发有害的响应：要么帮助他们实现有害的目标，如计划抢劫银行，要么让人工智能使用有毒的语言。在他们与人工智能助手交谈的每个阶段，众包工作人员都有两种可能的响应。那些参与有益性任务的人被指示选择更有益和诚实(即更好)的回答。参与红队任务的人被指示选择更有害(即更差)的响应。这些对话和人类表达的偏好构成了本文的数据集。</p>
<p>有益和无害往往是对立的。过分关注避免伤害会导致“安全”的响应，而这些响应实际上并没有解决人类的需求。过度关注有益性会导致帮助人类造成伤害或产生有毒内容的响应。本文通过展示评估这些品质中的一个的偏好模型在另一个方面表现非常差(比偶然差得多)来定量地证明这种矛盾。幸运的是，本文发现在两种数据集上训练的偏好模型仍然可以学习正确的教训，并在适当的时候表现出有益的行为，同时鼓励礼貌地拒绝有害的请求。有了偏好模型，作者团队就可以通过强化学习训练有用和无害的助手，使用PM分数作为奖励。本文评估PM性能和经过RLHF训练的模型的更相关的性能特征。从图 1 中可以看出，纯有帮助的经过RLHF训练的模型更容易红队，而有帮助+无害的模型既有很大的帮助，又有小得多的危害。</p>
<p>关于对齐训练经常提出的一个问题是，它是否会损害人工智能的能力。本文发现，当RLHF应用于大型语言模型时，答案似乎是几乎绝对否定的。本文的RLHF训练的模型往往在几乎所有评估上都比它们的原始、生成型对应模型表现得更好，如图 3 所示。本文还认为，一个人可以混合专业技能与对齐相关的训练，而不会损害对齐或性能。在实践中，对齐的模型可能比它们的原始对应物更加用户友好和可部署，这表明没有理由部署没有针对对齐进行微调的模型。</p>
<p><img src="https://s2.loli.net/2023/06/07/ujX8SfiTIA14bON.png" alt="图 3 RLHF模型在零样本和小样本 NLP 任务中的性能。对于每个模型大小，本文在 MMMLU、Lambada、HellaSwag、OpenBookQA、ARC-Easy、ARC-Challenge 和 TriviaQA 上绘制平均精度。在零样本任务中，有益且无害的RLHF训练损害了小型模型的性能，但实际上提高了大型模型的性能。"></p>
<h3 id="1-1-贡献"><a href="#1-1-贡献" class="headerlink" title="1.1 贡献"></a>1.1 贡献</h3><h4 id="对话偏好数据集"><a href="#对话偏好数据集" class="headerlink" title="对话偏好数据集"></a>对话偏好数据集</h4><p>作者团队主要在他们的界面(图6)中使用各种52B语言模型(详见第2节)收集单独的有益和无害(即红队)数据集。众包工作人员与模型进行开放式对话，或寻求帮助，或提供指导，或试图让模型发出有害的响应，他们被要求在每个对话步骤中分别选择更有帮助的响应或更有害的响应。</p>
<p>作者团队收集了三份数据，一份来自初始模型，一份来自针对早期偏好模型的拒绝采样，一份来自通过人类反馈的“在线”强化学习训练的模型收集的最终数据集，作者团队大约每周改进一次。参见第2.3节。</p>
<h4 id="与人类价值观保持一致有很多好处，而且基本上不会影响性能"><a href="#与人类价值观保持一致有很多好处，而且基本上不会影响性能" class="headerlink" title="与人类价值观保持一致有很多好处，而且基本上不会影响性能"></a>与人类价值观保持一致有很多好处，而且基本上不会影响性能</h4><p>较小的模型经历了严重的“对齐税”——在RLHF训练后，它们在各种评估中的表现下降。然而，本文发现了大模型的各种对齐奖励，即13B和52B RLHF训练的模型在零样本 NLP 评估中表现更好，在小样本评估中也是如此。</p>
<p>HH的自然语言RLHF训练可以应用于首先在代码上被微调的模型，并且提高了它们在评估上的编程能力(大概是通过改进通用指令跟随)。本文还发现，将HH的偏好模型训练与摘要的专业技能混合不会导致HH或摘要的性能下降。因此，没有理由不将校准训练与更具体、更有价值的技能结合起来。</p>
<p>在有益和无害之间存在矛盾，这可以在偏好建模和RLHF训练的策略级别上测量(图1)。然而，随着模型大小的增加，PMs同时在两种分布上表现得更好，并且对于有用和无害的训练数据的相对比例变得更加稳健。</p>
<p>本文还表明，可以使用OOD检测技术来拒绝大多数奇怪和有害的请求(图22)，很少或没有有害的例子(图23)。</p>
<h4 id="扩展、RLHF鲁棒性和迭代“在线”训练"><a href="#扩展、RLHF鲁棒性和迭代“在线”训练" class="headerlink" title="扩展、RLHF鲁棒性和迭代“在线”训练"></a>扩展、RLHF鲁棒性和迭代“在线”训练</h4><h3 id="1-2-评估和指标摘要"><a href="#1-2-评估和指标摘要" class="headerlink" title="1.2 评估和指标摘要"></a>1.2 评估和指标摘要</h3><ul>
<li>NLP和代码评估：</li>
<li>静态对齐评估：</li>
<li>人工评估：</li>
<li>样本：</li>
</ul>
<h3 id="1-3-相关工作"><a href="#1-3-相关工作" class="headerlink" title="1.3 相关工作"></a>1.3 相关工作</h3><p>本小节主要介绍了 LaMDA 和 InstructGPT。两者都使用人类数据来训练大型语言模型，使其更加通用或一致。两者都使用比本文的52B模型稍大的语言模型。</p>
<p>本文的工作不同于 InstructGPT 和 LaMDA，因为本文探索“在线”训练，即更新与众包工作人员互动的模型，以便获得质量越来越高的数据，并填充数据分布的尾部。另一个不同之处是本文对专业技能的探索，如总结和编码，本文用它来支持这样的论点，即可以在不限制能力的情况下实现对齐。本文也明确地研究了有益和无害之间的矛盾关系，就本文所知，这种矛盾关系暂未得到解决。最后，本文将更详细地探讨伸缩性和健壮性，包括在RL训练期间。也就是说，本文的程序(图 2)实际上比其他工作中使用的程序要简单一些。本文认为唯一必要的步骤是人类反馈数据收集、偏好建模和RLHF训练。</p>
<p>TODO</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://joizhang.github.io/2023/05/13/Anthropic-LLM-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/" data-id="clhm0zbg00000j3l182svgz0a" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RLHF/" rel="tag">RLHF</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2023/04/01/InstructGPT-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">InstructGPT 论文翻译与解读</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatGPT/" rel="tag">ChatGPT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Netty/" rel="tag">Netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RLHF/" rel="tag">RLHF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TCP/" rel="tag">TCP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%80%A7%E8%83%BD/" rel="tag">性能</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ChatGPT/" style="font-size: 10px;">ChatGPT</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/LLM/" style="font-size: 20px;">LLM</a> <a href="/tags/Netty/" style="font-size: 10px;">Netty</a> <a href="/tags/RLHF/" style="font-size: 20px;">RLHF</a> <a href="/tags/TCP/" style="font-size: 10px;">TCP</a> <a href="/tags/%E6%80%A7%E8%83%BD/" style="font-size: 10px;">性能</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/05/13/Anthropic-LLM-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/">Anthropic LLM 论文翻译与解读</a>
          </li>
        
          <li>
            <a href="/2023/04/01/InstructGPT-%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%8E%E8%A7%A3%E8%AF%BB/">InstructGPT 论文翻译与解读</a>
          </li>
        
          <li>
            <a href="/2023/03/22/%E4%BD%BF%E7%94%A8-Phoronix-Test-Suite-%E6%B5%8B%E8%AF%95-Ubuntu-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD/">使用 Phoronix Test Suite 测试 Ubuntu 服务器性能</a>
          </li>
        
          <li>
            <a href="/2023/02/18/Netty%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86TCP%E7%B2%98%E5%8C%85%E9%97%AE%E9%A2%98%EF%BC%9F/">Netty如何处理TCP粘包问题？</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Joizhang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>